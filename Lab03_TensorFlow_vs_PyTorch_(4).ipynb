{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhinav6300219502/Tensor-flow-and-Pytorch/blob/main/Lab03_TensorFlow_vs_PyTorch_(4).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c1e84a1",
      "metadata": {
        "id": "5c1e84a1"
      },
      "source": [
        "# Lab 03: TensorFlow vs. PyTorch\n",
        "- Train a model on MNIST in both TensorFlow and PyTorch, convert to TFLite and ONNX.  \n",
        "- Use tf.GradientTape for Tensorflow custom training loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c58bba",
      "metadata": {
        "id": "a1c58bba"
      },
      "source": [
        "## TensorFlow Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "23ebc05e",
      "metadata": {
        "id": "23ebc05e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddbf2193-b2c9-4b4b-950b-4ce5f7170029"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.8560 - loss: 0.5071\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9560 - loss: 0.1531\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9684 - loss: 0.1099\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9751 - loss: 0.0881\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9786 - loss: 0.0709\n",
            "TF Training time: 22.74 seconds\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9677 - loss: 0.1044\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.08921346068382263, 0.9728000164031982]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to range [0, 1]\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Build a simple feedforward neural network\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),             # Input shape matching MNIST image size\n",
        "    tf.keras.layers.Flatten(),                         # Flatten 28x28 images to 1D vectors\n",
        "    tf.keras.layers.Dense(64, activation='relu'),      # Hidden layer with 64 neurons and ReLU activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')    # Output layer with 10 neurons (one per digit class)\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer and categorical crossentropy loss\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model and measure training time\n",
        "start = time.time()\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "end = time.time()\n",
        "print(f\"TF Training time: {end-start:.2f} seconds\")     # Print the training duration\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "model.evaluate(x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72743ab8",
      "metadata": {
        "id": "72743ab8"
      },
      "source": [
        "## Convert TensorFlow model to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "be6ab50a",
      "metadata": {
        "id": "be6ab50a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50fe9675-f4bd-4eec-aa8f-52151a7fedca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpi671h43k'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor_15')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  138931313851472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138931313849936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138934006789136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138934006776848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(\"model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57c00c95",
      "metadata": {
        "id": "57c00c95"
      },
      "source": [
        "## PyTorch Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "623dfb49",
      "metadata": {
        "id": "623dfb49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a12df3db-3c4b-4bbb-b6f4-30baa31359de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Training time: 25.99 seconds\n",
            "Test accuracy: 0.9721\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "\n",
        "# Transform: Convert to tensor and flatten 28x28 image to a 784-dimensional vector\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.view(-1))  # Flatten the image\n",
        "])\n",
        "\n",
        "# Data loaders with different batch sizes\n",
        "train_loader = DataLoader(\n",
        "    datasets.MNIST(root='./data', train=True, transform=transform, download=True),\n",
        "    batch_size=64,  # Changed batch size\n",
        "    shuffle=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    datasets.MNIST(root='./data', train=False, transform=transform, download=True),\n",
        "    batch_size=512  # Changed batch size\n",
        ")\n",
        "\n",
        "# Define a deeper fully connected neural network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)    # More neurons in first layer\n",
        "        self.fc2 = nn.Linear(256, 64)     # Added a new hidden layer\n",
        "        self.fc3 = nn.Linear(64, 10)      # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x))     # LeakyReLU instead of ReLU\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        return self.fc3(x)                # Final layer (logits)\n",
        "\n",
        "# Instantiate model, loss function, and optimizer\n",
        "model = Net()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.8)  # SGD instead of Adam\n",
        "\n",
        "# Training loop\n",
        "start = time.time()\n",
        "for epoch in range(3):  # Fewer epochs\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "end = time.time()\n",
        "print(f\"PyTorch Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        output = model(x)\n",
        "        pred = output.argmax(1)\n",
        "        correct += (pred == y).sum().item()\n",
        "print(f\"Test accuracy: {correct / len(test_loader.dataset):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6dbdab0",
      "metadata": {
        "id": "f6dbdab0"
      },
      "source": [
        "## Convert PyTorch model to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ONNX\n",
        "!pip install onnx"
      ],
      "metadata": {
        "id": "WuMKMhHc8aLF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01ad263d-ee5a-47f0-d058-91fef8e312fb"
      },
      "id": "WuMKMhHc8aLF",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (1.18.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "09925e9a",
      "metadata": {
        "id": "09925e9a"
      },
      "outputs": [],
      "source": [
        "dummy_input = torch.randn(1, 784)\n",
        "torch.onnx.export(model, dummy_input, \"model.onnx\",\n",
        "                  input_names=[\"input\"], output_names=[\"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TensorFlow custom training loop using tf.GradientTape"
      ],
      "metadata": {
        "id": "sv4P-dSS_GQB"
      },
      "id": "sv4P-dSS_GQB"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Updated batch size\n",
        "batch_size = 128\n",
        "\n",
        "# Prepare datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) \\\n",
        "    .shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Define a deeper model with different activations\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='tanh'),     # First hidden layer with tanh\n",
        "    tf.keras.layers.Dense(64, activation='relu'),      # Second hidden layer with ReLU\n",
        "    tf.keras.layers.Dense(10, activation='softmax')    # Output layer\n",
        "])\n",
        "\n",
        "# Set up training components with RMSprop optimizer\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "# Custom training loop\n",
        "epochs = 3\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch, training=True)\n",
        "            loss = loss_fn(y_batch, logits)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        train_acc_metric.update_state(y_batch, logits)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch + 1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Custom evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ],
      "metadata": {
        "id": "KH-sDlHq_Gdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26e9bb08-dabe-412f-bba9-7d431a727ad4"
      },
      "id": "KH-sDlHq_Gdw",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n",
            "Step 0, Loss: 2.2753, Accuracy: 0.1875\n",
            "Step 100, Loss: 0.2774, Accuracy: 0.8443\n",
            "Step 200, Loss: 0.2559, Accuracy: 0.8791\n",
            "Step 300, Loss: 0.1419, Accuracy: 0.8955\n",
            "Step 400, Loss: 0.2378, Accuracy: 0.9055\n",
            "Training Accuracy for epoch 1: 0.9117\n",
            "\n",
            "Epoch 2/3\n",
            "Step 0, Loss: 0.2334, Accuracy: 0.9141\n",
            "Step 100, Loss: 0.2442, Accuracy: 0.9518\n",
            "Step 200, Loss: 0.1778, Accuracy: 0.9545\n",
            "Step 300, Loss: 0.1513, Accuracy: 0.9561\n",
            "Step 400, Loss: 0.1603, Accuracy: 0.9566\n",
            "Training Accuracy for epoch 2: 0.9578\n",
            "\n",
            "Epoch 3/3\n",
            "Step 0, Loss: 0.1161, Accuracy: 0.9922\n",
            "Step 100, Loss: 0.1103, Accuracy: 0.9678\n",
            "Step 200, Loss: 0.1525, Accuracy: 0.9682\n",
            "Step 300, Loss: 0.1562, Accuracy: 0.9691\n",
            "Step 400, Loss: 0.1067, Accuracy: 0.9687\n",
            "Training Accuracy for epoch 3: 0.9694\n",
            "\n",
            "TF Training time: 58.58 seconds\n",
            "Test Accuracy: 0.9707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Otimization with Graph Execution using @tf.function"
      ],
      "metadata": {
        "id": "E4Nlg4lb_qdW"
      },
      "id": "E4Nlg4lb_qdW"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Updated batch size\n",
        "batch_size = 64\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Define a deeper model with LeakyReLU and ReLU\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(256),\n",
        "    tf.keras.layers.LeakyReLU(negative_slope=0.1),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Updated optimizer to SGD\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "@tf.function\n",
        "def train_step(x_batch, y_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x_batch, training=True)\n",
        "        loss = loss_fn(y_batch, logits)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    train_acc_metric.update_state(y_batch, logits)\n",
        "    return loss\n",
        "\n",
        "# Training loop\n",
        "epochs = 4\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch + 1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ],
      "metadata": {
        "id": "Jmu_hciK_qle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c2640ac-f50c-4f75-a146-f06b06674ef2"
      },
      "id": "Jmu_hciK_qle",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/4\n",
            "Step 0, Loss: 2.3849, Accuracy: 0.0156\n",
            "Step 100, Loss: 1.6642, Accuracy: 0.4675\n",
            "Step 200, Loss: 1.0007, Accuracy: 0.6066\n",
            "Step 300, Loss: 0.6345, Accuracy: 0.6771\n",
            "Step 400, Loss: 0.6329, Accuracy: 0.7161\n",
            "Step 500, Loss: 0.7429, Accuracy: 0.7444\n",
            "Step 600, Loss: 0.5555, Accuracy: 0.7643\n",
            "Step 700, Loss: 0.4845, Accuracy: 0.7796\n",
            "Step 800, Loss: 0.3205, Accuracy: 0.7927\n",
            "Step 900, Loss: 0.3830, Accuracy: 0.8043\n",
            "Training Accuracy for epoch 1: 0.8082\n",
            "\n",
            "Epoch 2/4\n",
            "Step 0, Loss: 0.3873, Accuracy: 0.9062\n",
            "Step 100, Loss: 0.4358, Accuracy: 0.8957\n",
            "Step 200, Loss: 0.2541, Accuracy: 0.8965\n",
            "Step 300, Loss: 0.3790, Accuracy: 0.8957\n",
            "Step 400, Loss: 0.3437, Accuracy: 0.8957\n",
            "Step 500, Loss: 0.4167, Accuracy: 0.8961\n",
            "Step 600, Loss: 0.5236, Accuracy: 0.8965\n",
            "Step 700, Loss: 0.2462, Accuracy: 0.8965\n",
            "Step 800, Loss: 0.2277, Accuracy: 0.8984\n",
            "Step 900, Loss: 0.4395, Accuracy: 0.9006\n",
            "Training Accuracy for epoch 2: 0.9010\n",
            "\n",
            "Epoch 3/4\n",
            "Step 0, Loss: 0.1471, Accuracy: 0.9375\n",
            "Step 100, Loss: 0.3394, Accuracy: 0.9115\n",
            "Step 200, Loss: 0.3162, Accuracy: 0.9155\n",
            "Step 300, Loss: 0.2607, Accuracy: 0.9157\n",
            "Step 400, Loss: 0.2504, Accuracy: 0.9154\n",
            "Step 500, Loss: 0.4003, Accuracy: 0.9142\n",
            "Step 600, Loss: 0.2428, Accuracy: 0.9149\n",
            "Step 700, Loss: 0.3540, Accuracy: 0.9150\n",
            "Step 800, Loss: 0.1300, Accuracy: 0.9151\n",
            "Step 900, Loss: 0.2041, Accuracy: 0.9165\n",
            "Training Accuracy for epoch 3: 0.9169\n",
            "\n",
            "Epoch 4/4\n",
            "Step 0, Loss: 0.3461, Accuracy: 0.9375\n",
            "Step 100, Loss: 0.4757, Accuracy: 0.9216\n",
            "Step 200, Loss: 0.2063, Accuracy: 0.9226\n",
            "Step 300, Loss: 0.3323, Accuracy: 0.9241\n",
            "Step 400, Loss: 0.2133, Accuracy: 0.9233\n",
            "Step 500, Loss: 0.2187, Accuracy: 0.9236\n",
            "Step 600, Loss: 0.2098, Accuracy: 0.9230\n",
            "Step 700, Loss: 0.2502, Accuracy: 0.9233\n",
            "Step 800, Loss: 0.0897, Accuracy: 0.9237\n",
            "Step 900, Loss: 0.2014, Accuracy: 0.9248\n",
            "Training Accuracy for epoch 4: 0.9251\n",
            "\n",
            "TF Training time: 16.92 seconds\n",
            "Test Accuracy: 0.9294\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}